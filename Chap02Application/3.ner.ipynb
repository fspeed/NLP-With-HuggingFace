{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a85b1c2d-c102-4628-bee4-5e4dbe8980f1",
   "metadata": {},
   "source": [
    "# 二、自然语言处理之模型应用----命名实体识别\n",
    "\n",
    "HuggingFace有一个巨大的模型库，其中一些是已经非常成熟的经典模型，这些模型即使不进行任何训练也能直接得出比较好的预测结果，也就是常说的Zero Shot Learning。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1d74be2-0901-4c3f-8295-815466a28fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e9c604-9033-49c9-ac19-5b2d1ebd7888",
   "metadata": {},
   "source": [
    "### 1) 下载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddcc68b8-43ae-471c-8f20-cfb173a439cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 下载模型\n",
    "#!HF_ENDPOINT=https://hf-mirror.com hf download dslim/bert-base-NER --local-dir ../models/dslim/bert-base-NER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5447cd6d-64c6-479a-b4a8-a58c3fd2ee13",
   "metadata": {},
   "source": [
    "### 2) 使用pipeline加载模型\n",
    "\n",
    "使用管道工具时，调用者需要做的只是告诉管道工具要进行的任务类型，管道工具会自动分配合适的模型，直接给出预测结果，如果这个预测结果对于调用者已经可以满足需求，则不再需要再训练。\n",
    "\n",
    "管道工具的API非常简洁，隐藏了大量复杂的底层代码，即使是非专业人员也能轻松使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee0dc1eb-b78f-417f-8d7f-dc79ceaab0ec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../models/dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cuda\n"
     ]
    }
   ],
   "source": [
    "# 加载命名实体识别模型\n",
    "from transformers import pipeline\n",
    "ner_pipe = pipeline(task=\"ner\", \n",
    "                    model=\"../models/dslim/bert-base-NER\",\n",
    "                   device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54eac18e-2b9e-4629-a5b7-301445fc7c26",
   "metadata": {},
   "source": [
    "### 3) 查看模型的配置信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "263f0c26-ed28-488e-b9ca-f118fc24024a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertConfig {\n",
      "  \"_num_labels\": 9,\n",
      "  \"architectures\": [\n",
      "    \"BertForTokenClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"O\",\n",
      "    \"1\": \"B-MISC\",\n",
      "    \"2\": \"I-MISC\",\n",
      "    \"3\": \"B-PER\",\n",
      "    \"4\": \"I-PER\",\n",
      "    \"5\": \"B-ORG\",\n",
      "    \"6\": \"I-ORG\",\n",
      "    \"7\": \"B-LOC\",\n",
      "    \"8\": \"I-LOC\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"B-LOC\": 7,\n",
      "    \"B-MISC\": 1,\n",
      "    \"B-ORG\": 5,\n",
      "    \"B-PER\": 3,\n",
      "    \"I-LOC\": 8,\n",
      "    \"I-MISC\": 2,\n",
      "    \"I-ORG\": 6,\n",
      "    \"I-PER\": 4,\n",
      "    \"O\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.57.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 查看模型的配置信息\n",
    "print(ner_pipe.model.config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c78d3e-d7f8-4980-8ceb-cc78861f999c",
   "metadata": {},
   "source": [
    "### 4) 使用模型预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9160501-28b3-492b-9dd3-a61d61522edc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'entity': 'B-ORG', 'score': np.float32(0.99735945), 'index': 1, 'word': 'Hu', 'start': 0, 'end': 2}\n",
      "{'entity': 'I-ORG', 'score': np.float32(0.98321056), 'index': 2, 'word': '##gging', 'start': 2, 'end': 7}\n",
      "{'entity': 'I-ORG', 'score': np.float32(0.9978242), 'index': 3, 'word': 'Face', 'start': 8, 'end': 12}\n",
      "{'entity': 'I-ORG', 'score': np.float32(0.9986395), 'index': 4, 'word': 'Inc', 'start': 13, 'end': 16}\n",
      "{'entity': 'B-LOC', 'score': np.float32(0.9994894), 'index': 11, 'word': 'New', 'start': 40, 'end': 43}\n",
      "{'entity': 'I-LOC', 'score': np.float32(0.99935216), 'index': 12, 'word': 'York', 'start': 44, 'end': 48}\n",
      "{'entity': 'I-LOC', 'score': np.float32(0.99955374), 'index': 13, 'word': 'City', 'start': 49, 'end': 53}\n",
      "{'entity': 'B-LOC', 'score': np.float32(0.7335543), 'index': 19, 'word': 'D', 'start': 79, 'end': 80}\n",
      "{'entity': 'I-ORG', 'score': np.float32(0.57430446), 'index': 20, 'word': '##UM', 'start': 80, 'end': 82}\n",
      "{'entity': 'I-ORG', 'score': np.float32(0.5355703), 'index': 21, 'word': '##BO', 'start': 82, 'end': 84}\n",
      "{'entity': 'B-LOC', 'score': np.float32(0.9925688), 'index': 28, 'word': 'Manhattan', 'start': 114, 'end': 123}\n",
      "{'entity': 'I-LOC', 'score': np.float32(0.99719846), 'index': 29, 'word': 'Bridge', 'start': 124, 'end': 130}\n"
     ]
    }
   ],
   "source": [
    "# 命名实体识别\n",
    "sequence = \"\"\"Hugging Face Inc. is a company based in New York City. Its headquarters are in DUMBO,\n",
    "therefore very close to the Manhattan Bridge which is visible from the window.\"\"\"\n",
    "\n",
    "for entity in ner_pipe(sequence):\n",
    "    print(entity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95db2d9e-e535-40ec-9df0-b577bcfa6481",
   "metadata": {},
   "source": [
    "### 5) 使用from_pretrained加载模型\n",
    "参考这个页面 https://huggingface.co/dslim/bert-base-NER\n",
    "\n",
    "或这个页面 https://hf-mirror.com/dslim/bert-base-NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6bb827d-770e-4a42-b4ef-9da6493e54be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../models/dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'entity': 'B-PER', 'score': np.float32(0.99889874), 'index': 4, 'word': 'Wolfgang', 'start': 11, 'end': 19}\n",
      "{'entity': 'B-LOC', 'score': np.float32(0.999503), 'index': 9, 'word': 'Berlin', 'start': 34, 'end': 40}\n",
      "{'entity': 'I-LOC', 'score': np.float32(0.7820654), 'index': 10, 'word': 'Germany', 'start': 41, 'end': 48}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"../models/dslim/bert-base-NER\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"../models/dslim/bert-base-NER\")\n",
    "\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "example = \"My name is Wolfgang and I live in Berlin Germany\"\n",
    "\n",
    "ner_results = nlp(example)\n",
    "for r in ner_results:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b168b6-126b-4fc1-a0d6-b658f662d0fa",
   "metadata": {},
   "source": [
    "### 6) 下面我想让输出更可控一些（即不使用pipeline这么傻瓜的函数）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00c6a85a-3754-417e-885b-4fe477ab51d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertTokenizerFast(name_or_path='../models/dslim/bert-base-NER', vocab_size=28996, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True, added_tokens_decoder={\n",
      "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      ")\n",
      "BertForTokenClassification(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSdpaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=768, out_features=9, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0936fd80-f197-4329-ab95-621a556f209c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../models/dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101,  1422,  1271,  1110, 14326,  1105,   146,  1686,  1107,  3206,\n",
      "          1860,   102]])\n",
      "tensor([[[ 6.3067, -0.3467, -1.0567, -0.7085, -1.0865, -0.8405, -1.8050,\n",
      "          -0.8185, -1.2252],\n",
      "         [ 9.2293, -1.1977, -2.5957,  0.0104, -1.9989, -1.5482, -2.3986,\n",
      "          -0.6703, -1.2614],\n",
      "         [ 9.3833, -1.0201, -2.1171, -0.2670, -1.1930, -2.5010, -1.7489,\n",
      "          -1.1891, -1.1439],\n",
      "         [ 9.3078, -0.9415, -1.9995, -0.2671, -1.2567, -2.6290, -1.8227,\n",
      "          -1.0745, -1.0053],\n",
      "         [-0.4231, -1.4356, -2.8459,  8.0095, -0.3620, -0.6428, -2.8158,\n",
      "          -0.0446, -2.0519],\n",
      "         [ 9.5702, -1.4722, -1.7606, -0.9493, -0.8142, -2.2278, -1.2958,\n",
      "          -1.1866, -1.4799],\n",
      "         [ 9.1578, -1.3360, -2.6826, -0.0208, -1.6509, -1.8847, -2.2323,\n",
      "          -0.6267, -1.2597],\n",
      "         [ 9.7792, -1.1190, -2.5631, -1.0737, -1.6784, -1.8619, -1.6596,\n",
      "          -0.7448, -1.4209],\n",
      "         [ 9.7693, -0.6543, -2.6917, -1.0276, -2.2436, -1.7990, -1.9699,\n",
      "          -0.2894, -1.2642],\n",
      "         [ 0.0144, -0.6804, -2.2383, -0.5640, -2.0922, -1.0016, -1.6826,\n",
      "           8.7226, -1.7135],\n",
      "         [-1.6564, -1.0573, -0.9747, -2.6560, -1.0594, -2.6827, -0.8524,\n",
      "           4.8019,  6.0946],\n",
      "         [ 6.2227, -0.0770, -1.5189,  0.0892, -1.1986, -2.3533, -1.8767,\n",
      "          -0.0534, -1.3732]]])\n",
      "{0: 'O', 1: 'B-MISC', 2: 'I-MISC', 3: 'B-PER', 4: 'I-PER', 5: 'B-ORG', 6: 'I-ORG', 7: 'B-LOC', 8: 'I-LOC'}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizerFast, BertForTokenClassification\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"../models/dslim/bert-base-NER\")\n",
    "model = BertForTokenClassification.from_pretrained(\"../models/dslim/bert-base-NER\")\n",
    "\n",
    "example = \"My name is Wolfgang and I live in Berlin Germany\"\n",
    "inputs = tokenizer(example, return_tensors=\"pt\")\n",
    "print(inputs.input_ids)\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "print(logits)\n",
    "print(model.config.id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79ff3d09-d348-42a3-afc6-fd76118afc85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]\t0\tO\n",
      "My\t0\tO\n",
      "name\t0\tO\n",
      "is\t0\tO\n",
      "Wolfgang\t3\tB-PER\n",
      "and\t0\tO\n",
      "I\t0\tO\n",
      "live\t0\tO\n",
      "in\t0\tO\n",
      "Berlin\t7\tB-LOC\n",
      "Germany\t8\tI-LOC\n",
      "[SEP]\t0\tO\n"
     ]
    }
   ],
   "source": [
    "for i, logit in enumerate(logits[0]):\n",
    "    predicted_class_id = logit.argmax().item()\n",
    "    print(tokenizer.decode(inputs.input_ids[0][i]), end='\\t')\n",
    "    print(predicted_class_id, end='\\t')\n",
    "    print(model.config.id2label[predicted_class_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d06b8ff-03c7-4454-9b50-d46196e9da47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
