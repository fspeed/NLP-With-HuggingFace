{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d454ea8b-2ab4-4067-8fea-64980b573cf8",
   "metadata": {},
   "source": [
    "# 四、使用评价指标工具\n",
    "\n",
    "在训练和测试一个模型时往往需要计算不同的评价指标，如正确率、查准率、查全率、F1值等，具体需要的指标往往和处理的数据集、任务类型有关。HuggingFace提供了统一的评价指标工具，能够将具体的计算过程隐藏，调用者只需提供计算结果，由评价指标工具给出评价指标。\n",
    "\n",
    "https://huggingface.co/docs/evaluate/index\n",
    "\n",
    "### 注意：以下代码可能因为网络原因无法正常调用\n",
    "参考这里 https://blog.csdn.net/misaki_min/article/details/132650725\n",
    "从 https://github.com/huggingface/evaluate 直接下载evaluate代码\n",
    "\n",
    "访问上面的github网站，获得下载地址，然后用git命令或者wget命令下载\n",
    "\n",
    "`git clone https://github.com/huggingface/evaluate.git`\n",
    "\n",
    "`wget https://github.com/huggingface/evaluate/archive/refs/heads/main.zip`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8618ea88-649c-42db-a558-c4369760c753",
   "metadata": {},
   "source": [
    "Evaluate库中的Metric(指标)、Comparison(比较)和Measurement(测量)是三种不同的评估工具，用于评估机器学习模型和数据集。它们之间的关系和区别如下：\n",
    "\n",
    "1. Metric(指标)：\n",
    "* 用途：用于评估模型的性能\n",
    "* 具体含义：指标通过将模型的预测结果与真实标签进行比较来衡量模型的表现\n",
    "* 示例：准确率、精确率、召回率、F1分数等\n",
    "* 目的：提供了对模型性能的定量评估，帮助衡量模型在特定任务上的表现\n",
    "2. Comparison(比较)：\n",
    "* 用途：用于比较两个模型之间的差异\n",
    "* 具体含义：比较工具将两个模型的预测结果与真实标签进行对比，计算它们之间的一致性或差异程度\n",
    "* 示例：一致性指标、相对误差等\n",
    "* 目的：帮助评估不同模型之间的性能差异，找到更好的模型或进行模型选择\n",
    "3. Measurement(测量)：\n",
    "* 用途：用于研究数据集的属性和特性\n",
    "* 具体含义：测量工具用于对数据集进行分析，探索数据集的结构、分布、偏差等方面的信息\n",
    "* 示例：数据集大小、样本分布、类别不平衡度等\n",
    "* 目的：提供对数据集的详细了解，帮助了解数据集的特点和潜在问题\n",
    "\n",
    "这三种评估工具在Evaluate库中各自独立，用于不同的评估目的。通过使用这些工具，可以全面评估和理解机器学习模型和数据集的表现和特点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d76d083f-71ea-4837-831c-3c883193d736",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a20f5001-a441-4562-acfd-88eb6ebea3c8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy is the proportion of correct predictions among the total number of cases processed. It can be computed with:\n",
      "Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
      " Where:\n",
      "TP: True positive\n",
      "TN: True negative\n",
      "FP: False positive\n",
      "FN: False negative\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 加载一个评价指标\n",
    "# 任何度量(metric), 比较(comparison), 和测量(measurement)工具，都使用'evaluate.load'函数加载:\n",
    "\n",
    "# 下面举一个精度度量例子，首先是加载\n",
    "#accuracy = evaluate.load(\"accuracy\") # 从网络加载\n",
    "accuracy = evaluate.load(\"../evaluate/metrics/accuracy\") # 从本地加载\n",
    "\n",
    "# 输出描述属性：\n",
    "print(accuracy.description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c263db6-945d-4958-a110-e68a0c89379c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy is the proportion of correct predictions among the total number of cases processed. It can be computed with:\n",
      "Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
      " Where:\n",
      "TP: True positive\n",
      "TN: True negative\n",
      "FP: False positive\n",
      "FN: False negative\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 如果你想确保你正在加载正确类型的求值（特别是如果有名称冲突），你可以显式地传递类型：\n",
    "accuracy = evaluate.load(\"../evaluate/metrics/accuracy\", module_type=\"metric\")\n",
    "print(accuracy.description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2be8b3d8-f676-4b18-ab8b-f59afb443449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "@article{scikit-learn,\n",
      "  title={Scikit-learn: Machine Learning in {P}ython},\n",
      "  author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.\n",
      "         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.\n",
      "         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and\n",
      "         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},\n",
      "  journal={Journal of Machine Learning Research},\n",
      "  volume={12},\n",
      "  pages={2825--2830},\n",
      "  year={2011}\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 你可以看到它描述了理论上 accuracy 是如何工作的。\n",
    "# 如果你在你的论文中使用这个指标，你想要正确地引用它。你可以查看引文属性：\n",
    "print(accuracy.citation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "980ba731-cddf-470a-9336-0fc80c6782c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'predictions': Value('int32'), 'references': Value('int32')}\n"
     ]
    }
   ],
   "source": [
    "# 在我们将 accuracy 或其他评价模块应用于用例之前，我们需要知道 accuracy 的输入格式是什么：\n",
    "print(accuracy.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac507243-6bac-499c-8d96-5affca6aafca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.5}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 计算评估模块分数的最简单方法是使用必要的输入直接调用compute（）。\n",
    "# 只需将特征中看到的输入传递给compute（）方法。\n",
    "# 计算一组数据的accuracy\n",
    "accuracy.compute(references=[0,1,0,1], predictions=[1,0,0,1])\n",
    "\n",
    "# 求值模块以字典的形式返回结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b331ac22-a495-40b5-b8a8-a90eda7ccc51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.5}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 在某些情况下，您可以迭代地构建预测，在这种情况下使用add()或add_batch()函数。\n",
    "# 逐个数据对计算\n",
    "for ref, pred in zip([0,1,0,1], [1,0,0,1]):\n",
    "    accuracy.add(references=ref, predictions=pred)\n",
    "accuracy.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "596bf7ba-6dae-4379-a93b-1b91462617e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.5}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 按照batch计算\n",
    "for refs, preds in zip([[0,1],[0,1]], [[1,0],[0,1]]):\n",
    "    accuracy.add_batch(references=refs, predictions=preds)\n",
    "accuracy.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660dabcb-8514-43f1-8807-fe3f0bbdce64",
   "metadata": {},
   "source": [
    "## 组合评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "110d33f3-1289-476d-9f19-e4e9932c9ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.6666666666666666, 'f1': 0.6666666666666666, 'precision': 1.0, 'recall': 0.5}\n"
     ]
    }
   ],
   "source": [
    "#clf_metrics = evaluate.combine([\"accuracy\", \"f1\", \"precision\", \"recall\"])\n",
    "#results = clf_metrics.compute(predictions=[0, 1, 0], references=[0, 1, 1])\n",
    "#print(results)\n",
    "clf_metrics = evaluate.combine([\"../evaluate/metrics/accuracy\", \"../evaluate/metrics/f1\", \"../evaluate/metrics/precision\", \"../evaluate/metrics/recall\"])\n",
    "results = clf_metrics.compute(predictions=[0, 1, 0], references=[0, 1, 1])\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfaeb75-169d-49af-b285-770be6bc79e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
