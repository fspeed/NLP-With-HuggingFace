{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5e35216-9beb-454f-9f2a-98273aa1fff8",
   "metadata": {},
   "source": [
    "# 二、使用编码工具\n",
    "\n",
    "## 1.　编码工具简介\n",
    "\n",
    "HuggingFace提供了一套统一的编码API，由每个模型各自提交实现。由于统一了API，所以调用者能快速地使用不同模型的编码工具。\n",
    "\n",
    "在学习HuggingFace的编码工具之前，先看一个示例的编码过程，以理解编码工具的工作过程。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589cec1d-abda-4236-93c1-170aaacd3d73",
   "metadata": {},
   "source": [
    "## 2. 编码工具工作流示意\n",
    "\n",
    "### 1) 定义字典\n",
    "\n",
    "文字是一个抽象的概念，不是计算机擅长处理的数据单元，计算机擅长处理的是数字运算，所以需要把抽象的文字转换为数字，让计算机能够做数学运算。\n",
    "\n",
    "为了把抽象的文字数字化，需要一个字典把文字或者词对应到某个数字。一个示意的字典如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f60b0504-5030-4f65-bc0c-0eea60a76d3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 字典\n",
    "vocab = {\n",
    "    '<SOS>': 0,\n",
    "    '<EOS>': 1,\n",
    "    'the': 2,\n",
    "    'quick': 3,\n",
    "    'brown': 4,\n",
    "    'fox': 5,\n",
    "    'jumps': 6,\n",
    "    'over': 7,\n",
    "    'a': 8,\n",
    "    'lazy': 9,\n",
    "    'dog': 10\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf6d9a5-6396-43ac-a9ef-df509005fd1f",
   "metadata": {},
   "source": [
    "## 2) 句子预处理\n",
    "\n",
    "在句子被分词之前，一般会对句子进行一些特殊的操作，例如把太长的句子截短，或在句子中添加首尾标识符等。\n",
    "\n",
    "在示例字典中，我们注意到除了一般的词之外，还有一些特殊符号，例如<SOS>和<EOS>，它们分别代表一个句子的开头和结束。把这两个特殊符号添加到句子上，代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "055f40fc-2af9-4a83-aa36-553a6a1c86ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SOS> the quick brown fox jumps over a lazy dog <EOS>\n"
     ]
    }
   ],
   "source": [
    "# 简单编码\n",
    "sent = 'the quick brown fox jumps over a lazy dog'\n",
    "sent = '<SOS> ' + sent + ' <EOS>'\n",
    "print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e959a359-e5b9-408b-a290-5cdff41997e9",
   "metadata": {},
   "source": [
    "## 3) 分词\n",
    "\n",
    "现在句子准备好了，接下来需要把句子分成一个一个的词。对于中文来讲，这是个复杂的问题，但是对于英文来讲这个问题比较容易解决，因为英文有自然的分词方式，即以空格来分词，代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "595f60a6-d0c1-4907-9271-a85d70c7d9e1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<SOS>', 'the', 'quick', 'brown', 'fox', 'jumps', 'over', 'a', 'lazy', 'dog', '<EOS>']\n"
     ]
    }
   ],
   "source": [
    "# 英文分词\n",
    "words = sent.split()\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17991132-ebb1-4939-877c-7ce18d388eae",
   "metadata": {},
   "source": [
    "对于中文来讲，分词的问题比较复杂，因为中文所有的字是连在一起写的，不存在一个自然的分隔符号。有很多成熟的工具能够做中文分词，例如jieba分词、LTP分词等，但是在本书中不会使用这些工具，因为HuggingFace的编码工具已经包括了分词这一步工作，由各个模型自行实现，对于调用者来讲这些工作是透明的，不需要关心具体的实现细节。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3261bb00-f02d-43b4-9dee-ded6e884ee88",
   "metadata": {},
   "source": [
    "### 4) 编码\n",
    "\n",
    "句子已按要求添加了首尾标识符，并且分割成了一个一个的单词，现在需要把这些抽象的单词映射为数字。因为已经定义好了字典，所以使用字典就可以把每个单词分别地映射为数字，代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "baa1dff6-2a49-4e11-8ae6-7310cb8e678d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1]\n"
     ]
    }
   ],
   "source": [
    "# 编码为数字\n",
    "encode = [vocab[i] for i in words]\n",
    "print(encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e061c278-f8e3-438d-88ca-6a5b9dba1dcc",
   "metadata": {},
   "source": [
    "## 3. 下载Tansformers编码工具\n",
    "\n",
    "因为编码工具包括配置文件、词典文件、预训练的模型文件和分词文件等，这些数据通常比较大，不会包含在transfomers工具包中，因此我们需要提前下载它。\n",
    "\n",
    "这里我们用到`google-bert/bert-base-chinese`，所以我们使用huggingface-cli工具，从hf-mirror镜像下载它。\n",
    "\n",
    "`bert-base-chinese`的主页为： https://huggingface.co/google-bert/bert-base-chinese\n",
    "\n",
    "或国内镜像： https://hf-mirror.com/google-bert/bert-base-chinese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "659c1577-821b-43c6-8467-0ac946a4a5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!HF_ENDPOINT=https://hf-mirror.com hf download google-bert/bert-base-chinese --local-dir ../models/google-bert/bert-base-chinese"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8649b053-1824-4031-b17c-e10698a7a16a",
   "metadata": {},
   "source": [
    "### 查看模型中的文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e47338c-8891-47a4-8772-92ffe5d3cfe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README.md\t    model.safetensors  tokenizer.json\n",
      "config.json\t    pytorch_model.bin  tokenizer_config.json\n",
      "flax_model.msgpack  tf_model.h5        vocab.txt\n"
     ]
    }
   ],
   "source": [
    "! ls ../models/google-bert/bert-base-chinese"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac359e5-6182-44c5-af2a-ebfddfafebd6",
   "metadata": {},
   "source": [
    "其中主要文件：\n",
    "\n",
    "1) 配置文件：config.json。\n",
    "2) 词典文件：vocab.txt。\n",
    "3) 预训练模型文件：model.safetensors、pytorch_model.bin和tf_model.h5。\n",
    "4) 分词文件：tokenizer.json和tokenizer_config.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85904098-8471-4590-8d5c-7460fe5d4a0b",
   "metadata": {},
   "source": [
    "### 加载编码工具"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35142299-2883-4735-8bf1-963a59a8dabf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fspeed/HuggingFace/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertTokenizer(name_or_path='../models/google-bert/bert-base-chinese', vocab_size=21128, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True, added_tokens_decoder={\n",
      "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 加载编码工具\n",
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('../models/google-bert/bert-base-chinese')\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d251c5a-a0ea-4f29-aaf6-8ab99d7b41a9",
   "metadata": {},
   "source": [
    "## 4. 准备实验数据\n",
    "\n",
    "这是一些中文的句子，以测试编码工具"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7c87172",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 准备实验数据\n",
    "sents = [\n",
    "    '你站在桥上看风景',\n",
    "    '看风景的人在楼上看你',\n",
    "    '明月装饰了你的窗子',\n",
    "    '你装饰了别人的梦'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f78920-8e4c-4326-836f-c22951730183",
   "metadata": {},
   "source": [
    "## 5. 基本的编码函数\n",
    "\n",
    "使用基本的编码函数编码\n",
    "\n",
    "这里调用了编码工具的encode()函数，这是最基本的编码函数，一次编码一个或者一对句子，在这个例子中，编码了一对句子。\n",
    "\n",
    "不是每个编码工具都有编码一对句子的功能，具体取决于不同模型的实现。在BERT中一般会编码一对句子，这和BERT的训练方式有关系。\n",
    "\n",
    "1) 参数text和text_pair分别为两个句子，如果只想编码一个句子，则可让text_pair传None。\n",
    "2) 参数truncation=True表明当句子长度大于max_length时，截断句子。\n",
    "3) 参数padding= 'max_length'表明当句子长度不足max_length时，在句子的后面补充PAD，直到max_length长度。\n",
    "4) 参数add_special_tokens=True表明需要在句子中添加特殊符号。\n",
    "5) 参数max_length=25定义了max_length的长度。\n",
    "6) 参数return_tensors=None表明返回的数据类型为list格式，也可以赋值为tf、pt、np，分别表示TensorFlow、PyTorch、NumPy数据格式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d484803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 872, 4991, 1762, 3441, 677, 4692, 7599, 3250, 102, 4692, 7599, 3250, 4638, 782, 1762, 3517, 677, 4692, 872, 102, 0, 0, 0, 0]\n",
      "[CLS] 你 站 在 桥 上 看 风 景 [SEP] 看 风 景 的 人 在 楼 上 看 你 [SEP] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "# 基本的编码函数\n",
    "out = tokenizer.encode(\n",
    "      text=sents[0], #第一个句子\n",
    "      text_pair=sents[1], #第二个句子\n",
    "      truncation=True, #当句子长度大于max_length时截断\n",
    "      padding='max_length', #一律补PAD,直到max_length长度\n",
    "      add_special_tokens=True, #是否增加特殊的tokens\n",
    "      max_length=25, #句子最大长度\n",
    "      return_tensors=None)#返回数据的类型，可取值tf、pt、np,默认为返回list\n",
    "print(out)\n",
    "print(tokenizer.decode(out))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ab9b81-bdc6-47ba-8554-1856ba643384",
   "metadata": {},
   "source": [
    "可以看到编码的输出为一个数字的list，这里使用了编码工具的decode()函数把这个list还原为分词前的句子。这样就可以看出编码工具对句子做了哪些预处理工作。\n",
    "\n",
    "从输出可以看出，编码工具把两个句子前后拼接在一起，中间使用[SEP]符号分隔，在整个句子的头部添加符号[CLS]，在整个句子的尾部添加符号[SEP]，因为句子的长度不足max_length，所以补充了4个[PAD]。\n",
    "\n",
    "另外从空格的情况也能看出，编码工具把每个字作为一个词。因为每个字之间都有空格，表明它们是不同的词，所以在BERT的实现中，中文分词处理比较简单，就是把每个字都作为一个词来处理。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7218366f-96cf-4012-88f7-b0ec4e99c217",
   "metadata": {},
   "source": [
    "## 6. 进阶的编码函数\n",
    "\n",
    "encode_plus()函数，这是一个进阶版的编码函数，它会返回更加复杂的编码结果。和encode()函数一样，encode_plus()函数也可以编码一个句子或者一对句子，在这个例子中，编码了一对句子。\n",
    "\n",
    "参数return_token_type_ids、return_attention_mask、return_special_tokens_mask、return_length表明需要返回相应的编码结果，如果指定为False，则不会返回对应的内容。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b36624c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids:  [101, 872, 4991, 1762, 3441, 677, 4692, 7599, 3250, 102, 4692, 7599, 3250, 4638, 782, 1762, 3517, 677, 4692, 872, 102, 0, 0, 0, 0]\n",
      "将文本解码: [CLS] 你 站 在 桥 上 看 风 景 [SEP] 看 风 景 的 人 在 楼 上 看 你 [SEP] [PAD] [PAD] [PAD] [PAD]\n",
      "token_type_ids:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]\n",
      "special_tokens_mask:  [1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n",
      "attention_mask:  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]\n",
      "length:  25\n"
     ]
    }
   ],
   "source": [
    "# 进阶的编码函数\n",
    "out = tokenizer.encode_plus(\n",
    "      text=sents[0],\n",
    "      text_pair=sents[1],\n",
    "      truncation=True, \n",
    "      padding='max_length', \n",
    "      add_special_tokens=True, \n",
    "      max_length=25, \n",
    "      return_tensors=None, \n",
    "      return_token_type_ids=True, #返回token_type_ids\n",
    "      return_special_tokens_mask=True, #返回special_tokens_mask 特殊符号标识\n",
    "      return_attention_mask=True, #返回attention_mask\n",
    "      return_length=True) #返回length 标识长度\n",
    "\n",
    "#input_ids 编码后的词\n",
    "#token_type_ids 第1个句子和特殊符号的位置是0,第2个句子的位置是1\n",
    "#special_tokens_mask 特殊符号的位置是1,其他位置是0\n",
    "#attention_mask PAD的位置是0,其他位置是1\n",
    "#length 返回句子长度\n",
    "\n",
    "print('input_ids: ', out['input_ids'])\n",
    "print('将文本解码:', tokenizer.decode(out['input_ids']))\n",
    "\n",
    "print('token_type_ids: ', out['token_type_ids'])\n",
    "print('special_tokens_mask: ', out['special_tokens_mask'])\n",
    "print('attention_mask: ', out['attention_mask'])\n",
    "print('length: ', out['length'])\n",
    "\n",
    "#for k, v in out.items():\n",
    "#    print(k, ':', v)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056a307d-f105-409f-b43b-96549796bfeb",
   "metadata": {},
   "source": [
    "看第二行，把编码结果中的input_ids还原为文字形式，可以看到经过预处理的原文本。预处理的内容和encode()函数一致。\n",
    "\n",
    "这次编码的结果和encode()函数不一样的地方在于，这次返回的不是一个简单的list，而是4个list和1个数字，接下来对编码的结果分别进行说明。\n",
    "\n",
    "1) 输出input_ids：编码后的词，也就是encode()函数的输出。\n",
    "2) 输出token_type_ids：因为编码的是两个句子，这个list用于表明编码结果中哪些位置是第1个句子，哪些位置是第2个句子。具体表现为，第2个句子的位置是1，其他位置是0。\n",
    "3) 输出special_tokens_mask：用于表明编码结果中哪些位置是特殊符号，具体表现为，特殊符号的位置是1，其他位置是0。\n",
    "4) 输出attention_mask：用于表明编码结果中哪些位置是PAD。具体表现为，PAD的位置是0，其他位置是1。\n",
    "5) 输出length：表明编码后句子的长度。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683de972-eeba-4f5a-a20a-5f041e356f43",
   "metadata": {},
   "source": [
    "## 7. 批量编码函数\n",
    "\n",
    "以上介绍的函数，都是一次编码一对或者一个句子，在实际工程中需要处理的数据往往是成千上万的，为了提高效率，可以使用batch_encode_plus ()函数批量地进行数据处理。\n",
    "\n",
    "代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dce16cc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids : [[101, 872, 4991, 1762, 3441, 677, 4692, 7599, 3250, 102, 4692, 7599, 3250, 4638, 782, 1762, 3517, 677, 4692, 872, 102, 0, 0, 0, 0], [101, 3209, 3299, 6163, 7652, 749, 872, 4638, 4970, 2094, 102, 872, 6163, 7652, 749, 1166, 782, 4638, 3457, 102, 0, 0, 0, 0, 0]]\n",
      "token_type_ids : [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0]]\n",
      "special_tokens_mask : [[1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]]\n",
      "length : [21, 20]\n",
      "attention_mask : [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[CLS] 你 站 在 桥 上 看 风 景 [SEP] 看 风 景 的 人 在 楼 上 看 你 [SEP] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 批量编码成对的句子\n",
    "out = tokenizer.batch_encode_plus(\n",
    "      batch_text_or_text_pairs=[(sents[0], sents[1]), (sents[2], sents[3])],\n",
    "      add_special_tokens=True,\n",
    "      truncation=True, #当句子长度大于max_length时截断\n",
    "      padding='max_length', #一律补零,直到max_length长度\n",
    "      max_length=25,\n",
    "      return_tensors=None, #可取值tf、pt、np,默认为返回list\n",
    "      return_token_type_ids=True, #返回token_type_ids\n",
    "      return_attention_mask=True, #返回attention_mask\n",
    "      return_special_tokens_mask=True, #返回special_tokens_mask 特殊符号标识\n",
    "      #return_offsets_mapping=True, #返回offsets_mapping 标识每个词的起止位置,这个参数只能BertTokenizerFast使用\n",
    "      return_length=True) #返回length 标识长度\n",
    "\n",
    "#input_ids 编码后的词\n",
    "#token_type_ids 第1个句子和特殊符号的位置是0,第2个句子的位置是1\n",
    "#special_tokens_mask 特殊符号的位置是1,其他位置是0\n",
    "#attention_mask PAD的位置是0,其他位置是1\n",
    "#length 返回句子长度\n",
    "for k, v in out.items():\n",
    "    print(k, ':', v)\n",
    "tokenizer.decode(out['input_ids'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc6e75c-e2d6-4e24-a34e-5fdf29002a11",
   "metadata": {},
   "source": [
    "可以看到，这里的输出都是二维的list了，表明这是一个批量的编码。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c9e989-c664-49e4-a3f9-fd75e843944e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 8. 使用自定义字典编码\n",
    "\n",
    "### 1) 首先直接调用encode看一下分词的结果\n",
    "\n",
    "可以看到“明月”被分成了两个词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "221dac48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 3209, 3299, 6163, 7652, 749, 872, 4638, 4970, 102]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[CLS] 明 月 装 饰 了 你 的 窗 [SEP]'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 编码新添加的词\n",
    "out = tokenizer.encode(\n",
    "      text='明月装饰了你的窗子[EOS]',\n",
    "      text_pair=None,\n",
    "      truncation=True, #当句子长度大于max_length时截断\n",
    "      padding='max_length', #一律补PAD,直到max_length长度\n",
    "      add_special_tokens=True,\n",
    "      max_length=10,\n",
    "      return_tensors=None)\n",
    "print(out)\n",
    "tokenizer.decode(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61adcbd-ec8f-4ee8-a88d-1684ef09048f",
   "metadata": {},
   "source": [
    "### 2) 获取字典，然后输出结果\n",
    "\n",
    "可以看到，字典本身是个dict类型的数据。在BERT的字典中，共有21 128个词，并且“明月”这个词并不存在于字典中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc9c22e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab的类型： <class 'dict'>\n",
      "词表大小： 21128\n",
      "'明月'是否在词表内： False\n",
      "'明'是否在词表内： True\n",
      "'明'的序号： 3209\n",
      "'月'的序号： 3299\n"
     ]
    }
   ],
   "source": [
    "# 获取字典\n",
    "vocab = tokenizer.get_vocab()\n",
    "print(\"vocab的类型：\", type(vocab))\n",
    "print(\"词表大小：\", len(vocab))\n",
    "print(\"'明月'是否在词表内：\", '明月' in vocab)\n",
    "print(\"'明'是否在词表内：\", '明' in vocab)\n",
    "print(\"'明'的序号：\", vocab['明'])\n",
    "print(\"'月'的序号：\", vocab['月'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf4fc59-2582-4dee-84ec-e17c10b1c368",
   "metadata": {},
   "source": [
    "### 3) 添加新词\n",
    "\n",
    "既然“明月”并不存在于字典中，可以把这个新词添加到字典中\n",
    "\n",
    "这里添加了3个新词，分别为“明月”“装饰”和“窗子”。也可以添加新的符号"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac6ee906",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 添加新词\n",
    "tokenizer.add_tokens(new_tokens=['明月', '装饰', '窗子'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "23844fc1-c9fd-475f-87c0-32cfc6437fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "词表大小： 21131\n",
      "'明月'是否在词表内： True\n",
      "'明月'的序号： 21128\n",
      "'装饰'的序号： 21129\n",
      "'窗子'的序号： 21130\n"
     ]
    }
   ],
   "source": [
    "# 获取字典\n",
    "vocab = tokenizer.get_vocab()\n",
    "print(\"词表大小：\", len(vocab))\n",
    "print(\"'明月'是否在词表内：\", '明月' in vocab)\n",
    "print(\"'明月'的序号：\", vocab['明月'])\n",
    "print(\"'装饰'的序号：\", vocab['装饰'])\n",
    "print(\"'窗子'的序号：\", vocab['窗子'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "693d6d09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 添加新符号\n",
    "tokenizer.add_special_tokens({'eos_token': '[EOS]'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da2e8ba-8bc2-4013-8e05-fa0a3375f552",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 4) 再次编码查看分词结果\n",
    "\n",
    "可以看到，“明月”已经被识别为一个词，而不是两个词，新的特殊符号[EOS]也被正确识别。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3b1118cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 21128, 21129, 749, 872, 4638, 21130, 21131, 102, 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[CLS] 明月 装饰 了 你 的 窗子 [EOS] [SEP] [PAD]'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 编码新添加的词\n",
    "out = tokenizer.encode(\n",
    "      text='明月装饰了你的窗子[EOS]',\n",
    "      text_pair=None,\n",
    "      truncation=True, #当句子长度大于max_length时截断\n",
    "      padding='max_length', #一律补PAD,直到max_length长度\n",
    "      add_special_tokens=True,\n",
    "      max_length=10,\n",
    "      return_tensors=None)\n",
    "print(out)\n",
    "tokenizer.decode(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fa6761-a0a6-426b-b405-30fa18683536",
   "metadata": {},
   "source": [
    "## 9. 关于Tokenizer的思考\n",
    "先用tokenizer分词，看看分词后都得到了什么"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d4981a27-5d2e-4941-8694-01bd3c915c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101,   100,  9064,  8307,  9392,  8798, 10800,  8169, 10112,  8291,\n",
      "          8228,   100,  8330,  8174,   162,  8332, 10006,  8177,  9794,  8221,\n",
      "         10551, 12772,  8187,   119,   100,  9510,  8233, 11643,  9586, 10600,\n",
      "          8168,   119,   102]])\n",
      "[CLS]\n",
      "[UNK]\n",
      "an\n",
      "email\n",
      "ap\n",
      "##ol\n",
      "##og\n",
      "##i\n",
      "##zi\n",
      "##ng\n",
      "to\n",
      "[UNK]\n",
      "for\n",
      "the\n",
      "t\n",
      "##ra\n",
      "##gi\n",
      "##c\n",
      "garden\n",
      "##ing\n",
      "mi\n",
      "##sha\n",
      "##p\n",
      ".\n",
      "[UNK]\n",
      "how\n",
      "it\n",
      "ha\n",
      "##pp\n",
      "##ene\n",
      "##d\n",
      ".\n",
      "[SEP]\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Write an email apologizing to Sarah for the tragic gardening mishap. Explain how it happened.\"\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "print(input_ids)\n",
    "for id in input_ids[0]:\n",
    "    print(tokenizer.decode(id))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfef446-67b7-4711-b122-8b33406fb660",
   "metadata": {},
   "source": [
    "#### 为什么会有那么多的#号？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f186d9-2dbd-4e61-8c8a-3cc19254e36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.decode(9392))\n",
    "print(tokenizer.decode(8798))\n",
    "print(tokenizer.decode(10800))\n",
    "print(tokenizer.decode(8169))\n",
    "print(tokenizer.decode(10112))\n",
    "print(tokenizer.decode(8291))\n",
    "\n",
    "print(tokenizer.decode([9392,  8798, 10800,  8169, 10112,  8291]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569f17ef-7036-469a-b91d-fa8fccc88a13",
   "metadata": {},
   "source": [
    "#### 让我们将几个不同的语言模型的分词结果用彩色显示出来看看\n",
    "\n",
    "这里出了用到上面的模型，`google-bert/bert-base-chinese`还用到了\n",
    "1. `google-bert/bert-base-uncased`\n",
    "2. `distilbert/distilbert-base-cased-distilled-squad`\n",
    "3. `distilbert/distilbert-base-uncased-finetuned-sst-2-english`\n",
    "4. `openai-community/gpt2`\n",
    "5. `Qwen/Qwen2-Audio-7B`\n",
    "\n",
    "用以下命令一起都下载了："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef948fd6-a780-4b55-a111-4af23b675cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!HF_ENDPOINT=https://hf-mirror.com hf download google-bert/bert-base-uncased --local-dir ../models/google-bert/bert-base-uncased\n",
    "#!HF_ENDPOINT=https://hf-mirror.com hf download distilbert/distilbert-base-cased-distilled-squad --local-dir ../models/distilbert/distilbert-base-cased-distilled-squad\n",
    "#!HF_ENDPOINT=https://hf-mirror.com hf download distilbert/distilbert-base-uncased-finetuned-sst-2-english --local-dir ../models/distilbert/distilbert-base-uncased-finetuned-sst-2-english\n",
    "#!HF_ENDPOINT=https://hf-mirror.com hf download openai-community/gpt2 --local-dir ../models/openai-community/gpt2\n",
    "#!HF_ENDPOINT=https://hf-mirror.com hf download Qwen/Qwen2-Audio-7B --local-dir ../models/Qwen/Qwen2-Audio-7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57163f4c-f8eb-43bd-8afa-7374e093af41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "colors_list = [\n",
    "'102;194;165', '252;141;98', '141;160;203',\n",
    "'231;138;195', '166;216;84', '255;217;47'\n",
    "]\n",
    "def show_tokens(sentence, tokenizer_name):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "    token_ids = tokenizer(sentence).input_ids\n",
    "    print(tokenizer.decode(token_ids))\n",
    "    for idx, t in enumerate(token_ids):\n",
    "        print(f'\\x1b[0;30;48;2;{colors_list[idx % len(colors_list)]}m' + tokenizer.decode(t) + '\\x1b[0m', end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4801bf-63bf-47cf-b217-f2889ce7baea",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Write an email apologizing to Sarah for the tragic gardening mishap. Explain how it happened. 明月装饰了你的窗子\"\n",
    "show_tokens(sentence, '../models/google-bert/bert-base-chinese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68aec740-0c92-4cab-aab0-917f9d62abd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Write an email apologizing to Sarah for the tragic gardening mishap. Explain how it happened. 明月装饰了你的窗子\"\n",
    "show_tokens(sentence, '../models/google-bert/bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27a6417-c088-4999-8643-27fadb7cc088",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Write an email apologizing to Sarah for the tragic gardening mishap. Explain how it happened. 明月装饰了你的窗子\"\n",
    "show_tokens(sentence, '../models/distilbert/distilbert-base-cased-distilled-squad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addf1651-a0f6-45b0-b448-65401f14b2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Write an email apologizing to Sarah for the tragic gardening mishap. Explain how it happened. 明月装饰了你的窗子\"\n",
    "show_tokens(sentence, '../models/distilbert/distilbert-base-uncased-finetuned-sst-2-english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452a5793-017a-4781-a8e4-ae9393126a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Write an email apologizing to Sarah for the tragic gardening mishap. Explain how it happened. 明月装饰了你的窗子\"\n",
    "show_tokens(sentence, '../models/openai-community/gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3172115-3562-4841-889a-1c8554d537b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Write an email apologizing to Sarah for the tragic gardening mishap. Explain how it happened. 明月装饰了你的窗子\"\n",
    "show_tokens(sentence, '../models/Qwen/Qwen2-Audio-7B')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9236c6d7-000a-415c-acb4-f55e9fa38fc0",
   "metadata": {},
   "source": [
    "#### 上面的乱码是怎么回事？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bbd870-643e-4c97-b05e-60a2b617515c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这样没有乱码\n",
    "sentence = \"明月装饰了你的窗子\"\n",
    "show_tokens(sentence, '../models/Qwen/Qwen2-Audio-7B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93f7efe-6642-41c8-a4fd-d0936a795bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这样就有乱码了\n",
    "sentence = \" 明月装饰了你的窗子\"\n",
    "show_tokens(sentence, '../models/Qwen/Qwen2-Audio-7B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761f6247-8d49-46a9-8948-80bf4052061c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 是不是空格造成的呢？\n",
    "sentence = \" 明\"\n",
    "show_tokens(sentence, '../models/Qwen/Qwen2-Audio-7B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9793f0f1-35bc-4ee6-8e0e-2384df99a1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 我们发现“ 明”被分成了两个token\n",
    "sentence = \" 明\"\n",
    "tokenizer = AutoTokenizer.from_pretrained('../models/Qwen/Qwen2-Audio-7B')\n",
    "token_ids = tokenizer(sentence).input_ids\n",
    "print(token_ids)\n",
    "#ord(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4e8319-16c7-4c23-ad2e-44f223a2cd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这两个token分别decode后得到三个字符（这三个字符毫无意义）\n",
    "print(ord(tokenizer.decode([38903])[0]))\n",
    "print(ord(tokenizer.decode([38903])[1]))\n",
    "print(ord(tokenizer.decode([236])[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65586b65-bf94-426e-88d6-554f749cfa0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 但是如果我们把这两个token一起decode，则生成了两个字符。我们去网上查一下uncode编码“明”字对应的就是26126\n",
    "# https://www.lddgo.net/string/cjk-unicode\n",
    "print(ord(tokenizer.decode([38903, 236])[0]))\n",
    "print(ord(tokenizer.decode([38903, 236])[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cde7b1-5a62-4ecf-81e9-e45e5e377bf1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
