{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5171642e-cc48-45c6-a257-7ff1732dee79",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 二、自然语言处理之模型应用----文本生成\n",
    "\n",
    "HuggingFace有一个巨大的模型库，其中一些是已经非常成熟的经典模型，这些模型即使不进行任何训练也能直接得出比较好的预测结果，也就是常说的Zero Shot Learning。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95e9c233-db98-4ec9-b7ac-a992e2585488",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bafad7a-7abe-41a0-b6b2-1f9ebbe2a373",
   "metadata": {},
   "source": [
    "### 1) 下载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7874a95-53ea-4925-bde8-89f32cf159e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下载模型\n",
    "#!HF_ENDPOINT=https://hf-mirror.com hf download openai-community/gpt2 --local-dir ../models/openai-community/gpt2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a5452e-c8a7-469e-955c-1809cd410359",
   "metadata": {},
   "source": [
    "### 2) 使用pipeline加载模型\n",
    "\n",
    "使用管道工具时，调用者需要做的只是告诉管道工具要进行的任务类型，管道工具会自动分配合适的模型，直接给出预测结果，如果这个预测结果对于调用者已经可以满足需求，则不再需要再训练。\n",
    "\n",
    "管道工具的API非常简洁，隐藏了大量复杂的底层代码，即使是非专业人员也能轻松使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9aedb7e8-c34a-46f0-896c-6b57208987b3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    }
   ],
   "source": [
    "# 文本生成\n",
    "from transformers import pipeline\n",
    "text_generator = pipeline(task=\"text-generation\",\n",
    "                          model=\"../models/openai-community/gpt2\",\n",
    "                          device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f5e658-cea4-4e2f-a09e-6c1998a36f11",
   "metadata": {},
   "source": [
    "### 3) 查看模型的配置信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c64f1c9-2b08-4a73-9414-ae475c6661ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.57.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text_generator.model.config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d169a0-ecb9-40e4-9603-4e4a694925ad",
   "metadata": {},
   "source": [
    "### 4) 使用模型预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c443e997-2213-4459-ad3a-115072e21871",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"Hello, I'm a student, I will not allow anyone to steal my work.\\n\\nI was shocked when I read that students were being targeted by the IRS in this case. I feel ashamed and embarrassed to be one of the victims of the IRS. This was not my experience.\\n\\nI am so sorry for this. I just want to make sure everyone understands that I am only doing what I can.\\n\\nThank you everyone for your support.\\n\\nJohn,\\n\\nThis is a very serious matter. It is my understanding that you are a student. My email address is john.k.chapman@gmail.com.\\n\\nI did not take this action as a threat. I have been in and out of the U.S. for over 10 years. My parents did not want me to be a threat to my parents. I do not believe that my parents and I can ever be safe from the IRS. I am very concerned.\\n\\nI will do everything I can to make sure that the law is fair and this case is not one of those cases that is going to go to trial.\\n\\nIn the meantime, I am going to continue to defend myself and my family.\\n\\nThank you,\\n\\nJohn\\n\\nThank you for your support.\\n\"}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 生成文本\n",
    "start_sentence=\"Hello, I'm a student, I will\"\n",
    "text_generator(start_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7194a94-b4dd-4020-ac56-476658bfb641",
   "metadata": {},
   "source": [
    "### 5) 生成多个可选文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eaa6f725-6116-4910-badd-eb6fe4a2c7e8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'generated_text': 'Hello, I\\'m a student, I will be attending you tomorrow, and I\\'ll get you to get up for the exam. I\\'ll be there for you, too.\\n\\n\"I\\'m, uh, you know what, I\\'m really sorry. I\\'m sorry you didn\\'t do a better job than me. I\\'m really sorry that I\\'m the only one who didn\\'t get to see you. I\\'ve got a lot of things to say, and I really don\\'t want to lose my job. I just want to get up for the exam.\"\\n\\n\"What?\"\\n\\n\"I don\\'t wanna lose my job. I want to get up for the exam. That\\'s all. I know, I know, I know. I\\'m going to be here tomorrow, which means I\\'ll be right there with you every minute of the exam, and I\\'ll be able to get you to do the test. And I\\'m going to be in the office, and I\\'m going to be getting up for your exam. So I\\'ll be, uh, I\\'ll be here, and I\\'ll be your teacher, and I\\'ll be able to take care of the things you did for me during your time at school. And I\\'m not going to be lecturing you. I\\'m going to'}\n",
      "{'generated_text': 'Hello, I\\'m a student, I will keep my eyes open for you. That\\'s what I was thinking back at the end of the day. Because I\\'m very sad about this situation, but I will continue to be a student and be a teacher to your students.\"\\n\\nHe then said that he is going to keep his eyes open for you.\\n\\n\"I am going to keep looking for you. I will keep my eye on you. I will always look for you. And I will always look for you. I will never stop looking for you. I will find you. I will always find you. I will always find you.\"\\n\\n\"I\\'m sorry, I understand. I\\'m sorry.\"\\n\\nHe then asked, \"You are going to tell me about this? I\\'m going to tell you about everything. I\\'m going to tell you about everything. I\\'m going to tell you about all this. I\\'m going to tell you about all this. I\\'m going to tell you about everything. I\\'m going to tell you about everything. I\\'m going to tell you about everything. I\\'m going to tell you about everything. I\\'m going to tell you about everything.\"\\n\\nHe then asked, \"What do you want, Mr. Gatto?\"\\n\\n\"'}\n",
      "{'generated_text': \"Hello, I'm a student, I will always go to school for my master's degree. I will go to college to be a lawyer or other law professional. I will always be a good student. But I can't go to college to do work for my master's degree. In order to know what I can do, I have to go to college. I'm doing my masters degree in economics. In order to know what's important for my life, I have to go to college to be a journalist. I will always be a good student. But I can't go to college to do work for my master's degree. In order to know what's important for my life, I have to go to college. I'm doing my masters degree in economics. In order to know what's important for my life, I have to go to college. I'm doing my masters degree in economics. In order to know what's important for my life, I have to go to college. I'm doing my master's degree in economics. In order to know what's important for my life, I have to go to college. I'm writing for me, I will always go to college. I'm writing for me, I will always go to college. I'm writing for myself, I will always go to\"}\n",
      "{'generated_text': \"Hello, I'm a student, I will teach you how to understand the game.\\n\\nBefore I go on, I want to give some background. I am the co-founder of the game industry company, GamesCom, which is run by the same people I was, but based in Sweden. It's a massive company that has about 3.5 million employees worldwide.\\n\\nI have a very clear relationship with my family and friends. They are very supportive and supportive of me. They are very kind and supportive of me. I am very excited to learn how to do games and be a great player.\\n\\nI'm very excited to be working with you.\\n\\nWe have been very fortunate to have a lot of people in our team. We are very active in the game industry and we have a lot of friends.\\n\\nWe are all about the community and we are all about making the games we want. We are all about putting quality play in the game.\\n\\nThis year we have a lot of people from all around the world. We are all about creating a game and our efforts are to make it good. We want to make a game that is fun and engaging.\\n\\nWe have a lot of fun with this. We have a lot of fun with the community.\\n\"}\n",
      "{'generated_text': 'Hello, I\\'m a student, I will write for this site, and I want you to know that I am a professor of philosophy and that the students of the university have no interest in me. I am going to take you to my class for a few days, and then I will write a talk in my class about what I have learned, what I need to improve, and what it means to be \"better\".\\n\\nThe course will be offered as part of the coursework.\\n\\nPlease note that due to the nature of this course, you will not be able to attend the lectures.\\n\\nPlease note that you will be required to complete an online survey on the course.\\n\\nThanks for your interest in the course.\\n\\nThank you,\\n\\nMike\\n\\nThe course is available to students of all levels and levels of experience.\\n\\nPlease note that this course is not for all students.\\n\\nPlease note that if you are interested in taking this course, please contact me at:\\n\\nMichele@sageandgadget.com'}\n"
     ]
    }
   ],
   "source": [
    "start_sentence=\"Hello, I'm a student, I will\"\n",
    "results = text_generator(start_sentence, num_return_sequences=5)\n",
    "for r in results:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2090fb13-772b-4b57-81a0-9fef7f4ee411",
   "metadata": {},
   "source": [
    "### 6) 使用from_pretrained加载模型\n",
    "\n",
    "参考这个页面 https://huggingface.co/openai-community/gpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19d1dbda-a779-47dc-bf1e-0aa473ba17c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I'm a student, I will be\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch\n",
    "\n",
    "# 加载 GPT-2 tokenizer 和语言模型\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('../models/openai-community/gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('../models/openai-community/gpt2')\n",
    "\n",
    "# 输入文本转换为 tokens\n",
    "start_sentence=\"Hello, I'm a student, I will\"\n",
    "encoded_input = tokenizer(start_sentence, return_tensors='pt')\n",
    "\n",
    "# 获取模型的输出（只计算当前输入的一步结果）\n",
    "outputs = model(**encoded_input)\n",
    "logits = outputs.logits\n",
    "\n",
    "# 选择概率最高的下一个 token\n",
    "next_token_id = torch.argmax(logits[:, -1, :], dim=-1).unsqueeze(0)\n",
    "\n",
    "# 将生成的 token 添加到原始输入\n",
    "generated_sequence = torch.cat([encoded_input['input_ids'], next_token_id], dim=1)\n",
    "\n",
    "# 解码生成的 tokens 为文本\n",
    "decoded_output = tokenizer.decode(generated_sequence[0], skip_special_tokens=True)\n",
    "\n",
    "# 打印生成的文本\n",
    "print(decoded_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09dece80-7866-4245-8ccf-f623a5498bf8",
   "metadata": {},
   "source": [
    "#### 上面代码这个只能生一个token。\n",
    "\n",
    "要生成更长的文本，可以手动进行逐步生成。你可以根据每个时间步的预测结果，将生成的标记添加到输入序列中，逐步扩展序列长度。这样可以实现比单次预测更长的文本输出。\n",
    "\n",
    "具体步骤如下：\n",
    "\n",
    "- 首先，给模型输入初始的文本。\n",
    "- 通过 model(**encoded_input) 预测下一个标记。\n",
    "- 将预测的标记拼接到输入文本中，作为新的输入，再次进行预测。\n",
    "- 重复这个过程，直到生成足够长的文本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0c0d636-f41e-4857-b602-a58f2ad74cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I'm a student, I will be studying for my degree. I'm not going to be able to go to college. I'm not going to be able to go to college. I'm not going to be able to go to college. I'm not going to be able to go to college. I'm not going to be able to go to college. I'm not going to be able to go to college. I'm not going to be able to go to college. I'm not going to be able to go to\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch\n",
    "\n",
    "# 加载本地的 GPT-2 tokenizer 和语言模型\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('../models/openai-community/gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('../models/openai-community/gpt2')\n",
    "\n",
    "# 输入文本转换为token\n",
    "start_sentence=\"Hello, I'm a student, I will\"\n",
    "encoded_input = tokenizer(start_sentence, return_tensors='pt')\n",
    "\n",
    "# 最大生成长度\n",
    "max_length = 100  # 可以根据需要调整\n",
    "\n",
    "# 逐步生成\n",
    "generated = encoded_input['input_ids']  # 初始输入的 token ids\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for _ in range(max_length):\n",
    "        # 获取模型输出\n",
    "        outputs = model(input_ids=generated)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # 选择最后一个时间步的logits，选择概率最大的标记\n",
    "        next_token_id = torch.argmax(logits[:, -1, :], dim=-1).unsqueeze(0)\n",
    "\n",
    "        # 将新生成的标记拼接到已有序列中\n",
    "        generated = torch.cat((generated, next_token_id), dim=1)\n",
    "\n",
    "        # 如果生成结束标记 `<|endoftext|>`，则停止生成\n",
    "        if next_token_id.item() == tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "# 解码生成的标记为文本\n",
    "decoded_output = tokenizer.decode(generated[0], skip_special_tokens=True)\n",
    "\n",
    "print(decoded_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f843ce8-a791-4917-a01e-8b0448654c46",
   "metadata": {},
   "source": [
    "#### 上面生成的文本里有很多重复。\n",
    "\n",
    "为了避免生成重复的内容，可以考虑 使用Top-k采样代替每次都选择概率最高的标记（argmax），可以使用采样策略来增加生成内容的多样性。\n",
    "\n",
    "例如，top-k 采样从前 k 个可能性最高的标记中随机选择。\n",
    "\n",
    "以下是如何将 top-k 采样添加到代码中的例子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aebfc0ec-4915-4001-977d-2bb3eb3f4c24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I'm a student, I will stay with you when you decide to leave.\n",
      "\n",
      "You'll be happy you did; when you decide that you don't mind leaving now, will be happy you did!\n",
      "\n",
      "The old guard at Equestria Police has been a bad influence on you. Some of his members like it, many of his customers are simply not very smart, others make it to the top jobs in the city.\n",
      "\n",
      "So you don't mind, and if you do come back, you'll be happy\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# 加载本地 GPT-2 模型和分词器\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('../models/openai-community/gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('../models/openai-community/gpt2')\n",
    "\n",
    "# 输入文本转换为token\n",
    "start_sentence=\"Hello, I'm a student, I will\"\n",
    "encoded_input = tokenizer(start_sentence, return_tensors='pt')\n",
    "\n",
    "# 最大生成长度\n",
    "max_length = 100  \n",
    "generated = encoded_input['input_ids']\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for _ in range(max_length):\n",
    "        # 获取模型输出\n",
    "        outputs = model(input_ids=generated)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # 选择最后一个时间步的logits\n",
    "        next_token_logits = logits[:, -1, :]\n",
    "\n",
    "        # 使用 top-k 采样策略\n",
    "        top_k = 50  # 设置top-k的值\n",
    "        top_k_logits, top_k_indices = torch.topk(next_token_logits, top_k)\n",
    "        probs = torch.softmax(top_k_logits, dim=-1)\n",
    "\n",
    "        # 根据概率分布采样\n",
    "        next_token_id = torch.multinomial(probs, num_samples=1)\n",
    "        next_token_id = top_k_indices.gather(dim=-1, index=next_token_id)\n",
    "\n",
    "        # 将新生成的标记拼接到已有序列中\n",
    "        generated = torch.cat((generated, next_token_id), dim=1)\n",
    "\n",
    "        # 如果生成结束标记 `<|endoftext|>`，则停止生成\n",
    "        if next_token_id.item() == tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "# 解码生成的标记为文本\n",
    "decoded_output = tokenizer.decode(generated[0], skip_special_tokens=True)\n",
    "print(decoded_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447bdb96-e910-459a-8548-272645ba6d0d",
   "metadata": {},
   "source": [
    "## 生成的文本不能到一个完整的段落后自动停止\n",
    "\n",
    "要实现生成文本在形成一个完整段落后自动停止，可以使用以下策略：\n",
    "\n",
    "1. 检测结束标记 (<|endoftext|>)\n",
    "GPT-2 通常会生成一个特殊的结束标记 (<|endoftext|>) 来标志文本结束。如果模型生成了这个标记，可以停止生成。不过这种方式依赖于模型是否会在合适的时间生成结束标记。如果模型未生成结束标记，就需要更多策略来判断。\n",
    "\n",
    "2. 基于标点符号的判断\n",
    "可以检测生成文本中的标点符号来判断段落是否完整。常见的段落结束标志包括句号（.）、换行符（\\n）等。当检测到一定数量的句号或换行符时，可以认为段落已经完成并停止生成。\n",
    "\n",
    "3. 设置段落长度限制\n",
    "通过对生成的字符或标记长度进行限制来间接控制段落的结束。设定一个合理的长度（例如，100-200个字符或标记），生成超过这个长度时自动停止生成。\n",
    "\n",
    "代码示例：以下示例展示了如何通过检测句号来自动结束生成："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2386026-ef93-425b-8e23-e8d9176e563f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I'm a student, I will pay the tuition and fees.\"\n",
      "\n",
      "After he is satisfied, he will go further and go through the next steps to pay off a loan.\n",
      "\n",
      "\"I need someone to finance the whole loan,\" he said.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# 加载本地 GPT-2 模型和分词器\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('../models/openai-community/gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('../models/openai-community/gpt2')\n",
    "\n",
    "# 输入文本转换为token\n",
    "start_sentence=\"Hello, I'm a student, I will\"\n",
    "encoded_input = tokenizer(start_sentence, return_tensors='pt')\n",
    "\n",
    "# 最大生成长度\n",
    "max_length = 200\n",
    "generated = encoded_input['input_ids']\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for _ in range(max_length):\n",
    "        # 获取模型输出\n",
    "        outputs = model(input_ids=generated)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # 选择最后一个时间步的logits\n",
    "        next_token_logits = logits[:, -1, :]\n",
    "\n",
    "        # 使用top-k采样策略生成新标记\n",
    "        top_k = 50\n",
    "        top_k_logits, top_k_indices = torch.topk(next_token_logits, top_k)\n",
    "        probs = torch.softmax(top_k_logits, dim=-1)\n",
    "        next_token_id = torch.multinomial(probs, num_samples=1)\n",
    "        next_token_id = top_k_indices.gather(dim=-1, index=next_token_id)\n",
    "\n",
    "        # 将新生成的标记拼接到已有序列中\n",
    "        generated = torch.cat((generated, next_token_id), dim=1)\n",
    "\n",
    "        # 解码生成的标记为当前文本\n",
    "        decoded_output = tokenizer.decode(generated[0], skip_special_tokens=True)\n",
    "        \n",
    "        # 检查是否生成了足够的完整句子\n",
    "        if decoded_output.count('.') >= 3:  # 检查句号的数量，判断是否生成了至少三句话\n",
    "            break\n",
    "\n",
    "        # 如果生成结束标记 `<|endoftext|>`，则停止生成\n",
    "        if next_token_id.item() == tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "# 最终输出完整段落\n",
    "print(decoded_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe0dcc3-7110-4cf4-9eff-e6ba8a7b64b8",
   "metadata": {},
   "source": [
    "## 上面的代码越来越复杂了，我们还可以使用model.generate，实现自动生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47f73ec3-9589-4ef4-97b8-31460f0b4e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I'm a student, I will teach you everything. (A) You are not going to have any time or any training with me on this subject. (B) I'm going to have some personal training with you, you will learn, and you will learn from me. (C) Don't give me advice. (D) The training with you is just my opinion, as far as you are concerned. I cannot provide training, and you are not allowed to talk to me. (E) You are not allowed to do any other exercises with me because, and I know, you are all different types of people. (F) The class schedule is just not perfect, and you are allowed to practice in your own way. I want you to learn from me when you are alone. (G) I'm happy to meet you, but what can I expect from you? (H) What could be better? (I) I am not going to spend money, and you are only paid to sit here and\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# 加载模型和分词器\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('../models/openai-community/gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('../models/openai-community/gpt2')\n",
    "\n",
    "# 输入文本转换为token\n",
    "start_sentence=\"Hello, I'm a student, I will\"\n",
    "encoded_input = tokenizer(start_sentence, return_tensors='pt')\n",
    "\n",
    "# 调用 model.generate\n",
    "output = model.generate(\n",
    "    input_ids=encoded_input['input_ids'],\n",
    "    max_new_tokens=200,  # 控制最大生成长度\n",
    "    eos_token_id=tokenizer.eos_token_id,  # 假如模型有endoftext标记\n",
    "    do_sample=True,  # 使用采样策略\n",
    "    top_k=50,        # top-k 采样策略\n",
    "    top_p=0.95       # top-p (nucleus) 采样策略\n",
    ")\n",
    "\n",
    "# 解码生成的文本\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# 打印生成的文本\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f605ee2-5f4a-463f-9578-9791e5d665f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
